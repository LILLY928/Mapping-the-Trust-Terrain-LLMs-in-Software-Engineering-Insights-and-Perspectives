# -*- coding: utf-8 -*-
"""Google_Scholar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j1W8kEdtULTyIV-hMwS_LUR78KcmK7aa

## Code crawling for Google Scholar

* Keyword: (“trust” OR “distrust” OR “trustworthiness”) AND (“SE” OR “Software Engineering”) AND (“LLM” OR “Large Language Model” OR “LLMs” OR “Large Language Models” OR “Deep Learning” OR “Machine Learning”)

* Additional Critiria
  * First 20 pages
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from ast import keyword

from google.colab import drive
drive.mount('/content/drive')

results = []
# springer's page number start with 1
i = 0 #pagination start

while i < 2000:
  try:
    url=('https://scholar.google.com/scholar?start='+str(i)
    +'&q=(%E2%80%9Ctrust%E2%80%9D+OR+%E2%80%9Cdistrust%E2%80%9D+OR+%E2%80%9Ctrustworthiness%E2%80%9D)+AND+(%E2%80%9CSE%E2%80%9D+OR+%E2%80%9CSoftware+Engineering%E2%80%9D)+AND+(%E2%80%9CLLM%E2%80%9D+OR+%E2%80%9CLarge+Language+Model%E2%80%9D+OR+%E2%80%9CLLMs%E2%80%9D+OR+%E2%80%9CLarge+Language+Models%E2%80%9D+OR+%E2%80%9CDeep+Learning%E2%80%9D+OR+%E2%80%9CMachine+Learning%E2%80%9D)'
    +'&hl=en&as_sdt=0,47&as_vis=1')
    content = requests.get(url).text
    page = BeautifulSoup(content, 'html.parser')

    for entry in page.find_all('div', attrs={"class": "gs_r gs_or gs_scl"}):
      title = entry.find('h3', attrs={'class': "gs_rt"})
      detail = entry.find('div', attrs={'class','gs_a'}).text.split('-')
      abst = entry.find('div', attrs={'class','gs_rs'})
      year = detail[-2].split(',')

      results.append({"title": title.text.strip(),
                      'url': title.a['href'],
                      'authors':detail[0],
                      'year':detail[-2].split(',')[-1],
                      'conference':detail[-2].split(',')[0],
                      'origin':detail[-1],
                      "PDF":entry.a['href'],
                      'abstract':abst.text.strip()
                      })
  except:
      print("An exception occurred")
      print(i)
  i+=10

GS_df = pd.DataFrame(results)

GS_df.head()

GS_df.to_csv('/content/drive/MyDrive/Paper_crawing/GS_data.csv', index=False)